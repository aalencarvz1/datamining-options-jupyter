import pandas as pd
import numpy as np
import pymysql
import statsmodels.formula.api as smf
from sqlalchemy import create_engine, text
import import_ipynb
import ipywidgets as widgets
from IPython.display import display
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
import statsmodels.api as sm
from sklearn.metrics import mean_squared_error
from controllers.etl.etl_controller import ETLController
from sklearn.preprocessing import MinMaxScaler
from controllers.correlation.correlation_controller import CorrelationController
from controllers.regression.regression_controller import RegressionController
import matplotlib.pyplot as plt
from controllers.utils.Utils import mostrar_dataframe_interativo

print("bibliotecas carregadas")


#configuracoes iniciais
%load_ext autoreload
%autoreload 2
pd.set_option('display.max_rows', None)
pd.options.display.float_format = '{:.6f}'.format

print("configuracoes iniciais concluidas")


# Conectar ao banco de dados

engine = create_engine('mysql+pymysql://root:masterkey@localhost/datamining')

print("conectado ao banco")


#ETL 1 - carregamento dos dados

#carregamento dos dados

# Query para selecionar derivativos (70,80)

query = """
SELECT 
    codneg,
    lifedays,  
    totneg,    
    preexe,
    preult,
    preultref,
    statusopcperc,    
    vlextrinseco
FROM 
    cotahist
WHERE 
    tpmerc in (70,80)   
order by
    1,2 desc
"""

#carrega os dados no dataframe
original_df = pd.read_sql(query, engine)
print(f"dados carregados")

# Fechar a conexão, pois ela nao é mais necessaria, os dados já estao no dataframe
engine.dispose()


#mostra os dados brutos

df = original_df.copy()
total_lines = df.shape[0]
print(f"o df contem {total_lines} linha")
mostrar_dataframe_interativo(df)


#ETL 2 - limpeza

df = df.dropna(subset=['totneg', 'vlextrinseco','lifedays'])
df = df[df['totneg'].notna() & df['totneg'] > 0 & df['vlextrinseco'].notna()]

print(f"restaram {df.shape[0]} de {total_lines} linhas apos etl 2 ({df.shape[0] / total_lines * 100} %)")




#ETL 3.1 - vlextrinseco pouco variado - detecção

df_vlextrinseco_avg = df.groupby('codneg',as_index=False)['vlextrinseco'].mean()
df_vlextrinseco_avg = df_vlextrinseco_avg.sort_values(by='vlextrinseco')

mostrar_dataframe_interativo(df_vlextrinseco_avg)



#ETL 3.2 - vlextrinseco pouco variado - exclusão

df_vlextrinseco_avg = df_vlextrinseco_avg[df_vlextrinseco_avg['vlextrinseco'] >= 0.10]
df = df[df['codneg'].isin(df_vlextrinseco_avg['codneg'])]

print(f"restaram {df.shape[0]} de {total_lines} linhas apos etl 3.2 ({df.shape[0] / total_lines * 100} %)")



#ETL 4.1 - outliers dias que menos ocorrem negocios - detecção

#agrupar por lifedays para detectar outliers de dias faltando para o vencimento que menos ocorrem negocios
total_neg = df['totneg'].sum()
print(f"total negocios entre todos os derivativos no ano: {total_neg}")
total_neg_lf_sums = df.groupby('lifedays',as_index=False)['totneg'].sum()
total_neg_lf_sums = total_neg_lf_sums.sort_values(by='totneg',ascending=False).reset_index(drop=False)
total_neg_lf_sums['pos'] = total_neg_lf_sums.index + 1
total_neg_lf_sums['rep (%)'] = total_neg_lf_sums['totneg'] / total_neg * 100
total_neg_lf_sums = total_neg_lf_sums.drop(columns=['index'])

mostrar_dataframe_interativo(total_neg_lf_sums)



#ETL 4.2 - outliers dias que menos ocorrem negocios - exclusao

#aplica etl´s lifedays outliers
print("eliminando lifedays < -85 do conjunto de dados")
df = df[df['lifedays'] >= -85]
df = df[df['totneg'] >= 5] #se no dia tiveram poucos negocios, significa que naquele dia os negocios nao refletiram o mercado

print(f"restaram {df.shape[0]} de {total_lines} linhas apos etl 4.2 ({df.shape[0] / total_lines * 100} %)")

mostrar_dataframe_interativo(df)



#ETL 5.1 - outliers derivativos que menos tem negocios comparado ao total - detecção

#agrupa por codneg e total negocios a fim de identificar outliers de derivativos que são pouco negociados comparados ao total
total_neg = df['totneg'].sum()
print(f"total negocios apos etl 1: {total_neg}")
total_neg_sums = df.groupby('codneg',as_index=False)['totneg'].sum()
total_neg_sums = total_neg_sums.sort_values(by='totneg',ascending=False).reset_index(drop=False)
total_neg_sums['pos'] = total_neg_sums.index + 1
total_neg_sums['rep (%)'] = total_neg_sums['totneg'] / total_neg * 100

mostrar_dataframe_interativo(total_neg_sums)



#ETL 5.2 - outliers derivativos que menos tem negocios comparado ao total - preparacao

#aplica etl sobre total de negocios por derivativos (% part)
total_derivativos_antes = total_neg_sums.shape[0]
total_neg_sums = total_neg_sums[total_neg_sums['rep (%)'] >= 0.01]
total_derivativos_apos = total_neg_sums.shape[0]

print(f"restaram {total_derivativos_apos} de {total_derivativos_antes} derivativos apos etl 5.2 ({total_derivativos_apos / total_derivativos_antes * 100} %)")



#ETL 5.2 - outliers derivativos que menos tem negocios comparado ao total - exclusao

#aplica etl sobre total de negocios por derivativos nos dados originais(% part)
df = df[df['codneg'].isin(total_neg_sums['codneg'])]
print(f"restaram {df.shape[0]} de {total_lines} linhas apos etl 5.2 ({df.shape[0] / total_lines * 100} %)")

mostrar_dataframe_interativo(df)



#ETL 6 - outliers derivativos que restaram poucos dias com negocios

df_count = df.groupby('codneg',as_index=False).size()
df_count = df_count[df_count['size'] >= 10]
df = df[df['codneg'].isin(df_count['codneg'])]

print(f"restaram {df.shape[0]} de {total_lines} linhas apos etl 6 ({df.shape[0] / total_lines * 100} %)")

#exportar
df.to_excel("dados_tratados.xlsx", index=False)




#DATAMINING - CORRELATION

#Efetua calculo de correlação entre lifedays e vlextrinseco nos dados filtrados, para cada derivativo (codneg)
correlation_controller = CorrelationController()
df_correlation = df.groupby(['codneg']).apply(
    lambda group: correlation_controller.calc_group_correlation(
        group[['vlextrinseco', 'lifedays']], 'vlextrinseco', 'lifedays'
    )
).reset_index(name='correlation')

df_correlation = df_correlation.sort_values(by='correlation',ascending=True).reset_index(drop=False)

print("correlação por derivativo entre lifedays e vlextrinseco")
mostrar_dataframe_interativo(df_correlation)
#pd.set_option('display.max_rows', None)
#print(df_correlation)


#DATAMINING - CORRELATION

# elimina do grupo derivativos cuja correlação não seja considerada negativamente forte (<= -0.75)
df_correlation_filtered = df_correlation[df_correlation['correlation'] <= -0.75]

mostrar_dataframe_interativo(df_correlation_filtered)



#DATAMINING - LINEAR REGRESSION

regression_controller = RegressionController()


#DATAMINING - LINEAR REGRESSION

def process_group_regression(group):
    regression_result = regression_controller.simpleLinearRegressionCalc(group, 'vlextrinseco', 'lifedays')
    regression_controller.plotSimpleLinearRegression(group,regression_result, f"Regressao Linear {group['codneg'].iloc[0]} ({regression_result['expression']})", 'Life days','vl extrinseco')


#DATAMINING - LINEAR REGRESSION

df_regression = df[df['codneg'].isin(df_correlation_filtered['codneg'])]
df_100 = df_regression[:100] #sao muitos registros para mostrar
df_100.groupby(['codneg']).apply(process_group_regression)



# Seleção das variáveis independentes e variável dependente
X = df[['lifedays', 'preultref', 'preexe']]
y = df['vlextrinseco']

# Separação dos dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Aplicação da transformação polinomial
poly = PolynomialFeatures(degree=2)  # Ajuste o grau conforme necessário
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# Treinamento do modelo de regressão linear com os dados transformados
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Predição e avaliação
y_pred = model.predict(X_test_poly)
mse = mean_squared_error(y_test, y_pred)

print("Erro Médio Quadrático (MSE):", mse)



# Usando apenas a coluna 'lifedais' como exemplo para o gráfico
X = df[['lifedays']]
y = df['vlextrinseco']

# Separação dos dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Transformação polinomial e treinamento do modelo
poly = PolynomialFeatures(degree=2)  # Ajuste o grau conforme necessário
X_train_poly = poly.fit_transform(X_train)
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Geração de valores para a linha de regressão
X_plot = np.linspace(X.min(), X.max(), 100)  # 100 pontos para suavidade
X_plot_poly = poly.transform(X_plot.reshape(-1, 1))
y_plot = model.predict(X_plot_poly)

# Plotagem
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', label="Dados reais")
plt.plot(X_plot, y_plot, color='red', label="Regressão Polinomial")
plt.xlabel('lifedays')
plt.ylabel('vlextrinseco')
plt.title('Regressão Polinomial de lifedais sobre vlextrinseco')
plt.legend()
plt.show()


from mpl_toolkits.mplot3d import Axes3D

# Selecionando duas variáveis independentes e a variável alvo
X = df[['lifedays', 'preultref']]
y = df['vlextrinseco']

# Separação dos dados e transformação polinomial
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train)
model = LinearRegression()
model.fit(X_train_poly, y_train)

# Criação de uma grade para o gráfico 3D
x_surf, y_surf = np.meshgrid(np.linspace(X['lifedays'].min(), X['lifedays'].max(), 100),
                             np.linspace(X['preultref'].min(), X['preultref'].max(), 100))
z_surf = model.predict(poly.transform(np.c_[x_surf.ravel(), y_surf.ravel()]))
z_surf = z_surf.reshape(x_surf.shape)

# Plotagem 3D
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(X['lifedays'], X['preultref'], y, color='blue', label="Dados reais")
ax.plot_surface(x_surf, y_surf, z_surf, color='red', alpha=0.3)
ax.set_xlabel('lifedays')
ax.set_ylabel('preultref')
ax.set_zlabel('vlextrinseco')
ax.set_title('Regressão Polinomial 3D')
plt.legend()
plt.show()


#ETL 7 - normalizacao

#normaliza as colunas de valores para que tenham a mesma proporção

cols_price = ["preexe","preult","preultref","vlextrinseco"]
unique_codneg = df['codneg'].unique()
for col in cols_price:
    min = df[col].min()
    max = df[col].max()
    scaler = MinMaxScaler(feature_range=(min, max))
    for cod in unique_codneg:
        df.loc[:, col] = scaler.fit_transform(df[[col]])

print("normalizacao concluida")
mostrar_dataframe_interativo(df)


# Supondo que `df` seja seu DataFrame original
unique_codneg = df['codneg'].unique()

# Ajustando e plotando o modelo de regressão múltipla para cada `codneg`
for cod in unique_codneg:
    # Filtra os dados para cada `codneg`
    df_cod = df[df['codneg'] == cod]
    
    # Define variáveis independentes e dependente
    X = df_cod[['lifedays', 'preult','preultref', 'preexe']]
    y = df_cod['vlextrinseco']
    
    # Adiciona uma constante ao modelo (intercepto)
    X = sm.add_constant(X)
    
    # Ajusta o modelo de regressão múltipla
    model = sm.OLS(y, X).fit()
    
    # Exibe o resumo do modelo para cada `codneg`
    print(f"Resultados para {cod}:")
    print(model.summary())
    
    # Prediz `vlextrinseco` para o intervalo de valores de `lifedays`
    y_pred = model.predict(X)
    
    # Plotagem do ajuste para o `codneg` atual
    plt.figure(figsize=(10, 6))
    plt.plot(df_cod['lifedays'], y, label='Dados reais', color='blue', marker='o')
    plt.plot(df_cod['lifedays'], y_pred, label='Regressão Ajustada', color='red')
    plt.xlabel('lifedays')
    plt.ylabel('vlextrinseco')
    plt.title(f'Ajuste de Regressão para codneg {cod}')
    plt.legend()
    plt.show()
